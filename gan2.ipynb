{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing PatMetheny_NothingPersonal_FINAL.middid.midL.middNAL.mid\r"
     ]
    }
   ],
   "source": [
    "resolution = 5\n",
    "length = 3000\n",
    "noterange = 64\n",
    "\n",
    "train_images = preprocessing.loadData(songLen=length, resolution=resolution, noterange=noterange)\n",
    "labels = np.zeros(len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "songLength = int(length / resolution)\n",
    "midiNotes = noterange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], songLength, midiNotes, 1).astype('float32')\n",
    "train_images = (train_images -0.5) * 2  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = [midiNotes, songLength, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    y = int(songLength/4)\n",
    "    x = int(midiNotes/4)\n",
    "    model.add(layers.Dense(x*y*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((y, x, 256)))\n",
    "    # assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    # assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    # assert model.output_shape == (None, songLength/2, midiNotes/2, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    print(model.output_shape)\n",
    "    assert model.output_shape == (None, songLength,midiNotes, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3000, 128, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14ecb2940>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[songLength, midiNotes, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00010155]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/matthewding/Desktop/Soph Year/Spring Quarter/CS221/GANnonball/gan2.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/matthewding/Desktop/Soph%20Year/Spring%20Quarter/CS221/GANnonball/gan2.ipynb#ch0000022?line=0'>1</a>\u001b[0m train(train_dataset, EPOCHS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(generator, f\"../models/gan_{EPOCHS}_generator.h5\")\n",
    "tf.keras.models.save_model(discriminator, f\"../models/gan_{EPOCHS}_discriminator.h5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f83d49ec2e377ff593a7e1f57e0bf435ba6c21cc1cd7298abcfa9e9cf3e4623f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
